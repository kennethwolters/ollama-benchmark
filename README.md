# ollama-benchmark
Assessing how LLM inference performance optimization could be done on ollama
